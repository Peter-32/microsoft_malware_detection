import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from fuzzywuzzy import fuzz
from pandasql import sqldf
import missingno as msno
from numpy import ceil, min
from pandas import read_csv
sns.set(style="whitegrid")
mpl.rcParams['figure.figsize'] = (16.0, 5.0)
def q(q): return sqldf(q, globals())

df = read_csv("/users/petermyers/data/microsoft_malware_prediction/train.csv")
df.head()


class UnderstandData:

    def in_excel(self):
        # display(spark_df)
        df.iloc[0:100].to_csv("/users/petermyers/work/microsoft_malware_detection/sample.csv")
        print("Sample written out.")

    def describe(self):
        print("Read documentation")
        print(df.dtypes)
        return df.describe()

    def skew(self):
        return df.shew()

    def heatmap(self):
        sns.heatmap(df.corr().apply(lambda x: abs(x)), vmax=.8, square=True)

    def lasso(self):
        lasso = Lasso()
        lasso.fit(X, y)
        print("coefficients: " + lasso.coef_)

# Read about each feature in the documentation.
UnderstandData().in_excel()

### Ctrl Option - ###

UnderstandData().describe()

### Ctrl Option - ###

UnderstandData().skew()

### Ctrl Option - ###

UnderstandData().heatmap()

### Ctrl Option - ###

UnderstandData().lasso()

### Ctrl Option - ###

class Dupes:

    def __init__(self):
        index_columns = 'col1, col2'

    def plot(self):
        plt.clf()
        df = spark.sql("""
                       SELECT
                       IF(number_of_rows=1, 'unique', 'dupe') AS type,
                       count(*) AS count
                       FROM
                       (SELECT {}, count(*) as number_of_rows
                           FROM spark_df
                           GROUP BY 1
                        ) x
                       GROUP BY 1
                       """.format(index_columns)).toPandas()
        sns.barplot(x="type", y="count", data=df)
        # display(plt.show())
        plt.show()

    def excel(self):
        # display(spark.sql("""
        #                   SELECT x.*
        #                   FROM
        #                   (SELECT x.*,
        #                    COUNT(*) OVER(PARTITION BY {}) AS number_of_rows
        #                       FROM spark_df x
        #                    ) x
        #                   ORDER BY number_of_rows DESC
        #                   """.format(index_columns)))
        df.duplicated(subset=['col1'])

# Dupes.plot()
# Dupes.excel()

### Ctrl Option - ###

class Missing:

    def plot():
        msno.matrix(df)

    def excel():
        display(spark.sql("""
                          SELECT *
                          FROM spark_df
                          WHERE col1 is null
                          """))


Missing().plot()
Missing().excel()

### Ctrl Option - ###


class Strange:

    def numeric_has_infinity_values(self):
        print(df.fillna(0).replace(
            [np.inf, -np.inf], np.nan).isnull().values.any())

    def numeric_plot(self):
        n_rows, n_columns = self._get_rows_and_column_counts(
            df, numeric_columns=True)
        plt.clf()
        fig = plt.figure(16.0, 4.0 * n_rows)
        i = 1
        for column in df.columns:
            if df[column].dtype in ('int64', 'float64'):
                ax = fig.add_subplot(n_rows, n_columns, i)
                sns.boxplot(x=df[column], ax=ax)
                i += 1
        plt.tight_layout()
        display(fig.figure)

    def category_count(self, df, nth_set_of_8=1):
        n_rows, n_columns = self._get_rows_and_column_counts(
            numeric_columns=False)
        plt.clf()
        fig = plt.figure(16.0, 4.0 * n_rows)
        i, n_to_skip = 1, (nth_set_of_8 - 1) * 8
        for column in df.columns:
            if df[column].dtype in ('bool', 'object'):
                if n_to_skip > 0:
                    n_to_skip -= 1
                else:
                    ax = fig.add_subplot(n_rows, n_columns, i)
                    df = df.sort_values(by=[column])
                    sns.countplot(x=column, data=df, ax=ax)
                    plt.xticks(rotation=20)
                    if i > 7:
                        break
                    i += 1
        plt.tight_layout()
        display(fig.figure)

    def set_uncommon_categories_to_other(self, df, threshold):
        category_values_to_keep = {}
        for column in df.columns:
            if df[column].dtype in ('bool', 'object'):
                value_count_result = pd.DataFrame(pd.value_counts(
                    df[column].values, sort=False, normalize=True), columns=[column + "_pct"]).reset_index()
                for index, row in value_count_result.iterrows():
                    if row[column + "_pct"] > threshold:
                        category_values_to_keep[row['index']] = ''
                for index, row in df.iterrows():
                    if row[column] not in category_values_to_keep:
                        df.at[index, column] = '_Other'
        return df

    def print_best_fuzzy_scores(self, df, nth_set_of_8=1):
        for column in df.columns:
            if df[column].dtype == "object":
                values = df[column].unique()
                values.sort()
                for i in range(len(values)):
                    for j in range(i + 1, len(values)):
                        score = fuzz.ratio(values[i], values[j])
                        if score > 35:
                            print("column: {}, score: {}, values {} & {}".format(
                                column, score, values[i], values[j]))

    def excel_sorted_category_values_and_count(self, df):
        temp_dfs = []
        for column in df.columns:
            if df[column].dtype in ('bool', 'object'):
                temp_df = pd.DataFrame(pd.value_counts(
                    df[column].values, sort=False, normalize=False), columns=["count"]).reset_index()
                temp_df['column'] = column
                temp_dfs.append(temp_df)
        new_df = pd.concat(temp_dfs)
        return display(spark.createDataFrame(q("SELECT column, index AS value, count FROM new_df ORDER BY 1 ASC, 3 DESC")))

    def _get_rows_and_column_counts(self, numeric_columns=True):
        count = 0
        for column in df.columns:
            if numeric_columns:
                if df[column].dtype in ('int64', 'float64'):
                    count += 1
            else:
                if df[column].dtype in ('bool', 'object'):
                    count += 1
        n_rows = min([2, ceil(count / 4)])
        n_columns = min([count, 4])
        return n_rows, n_columns

# Strange().numeric_has_infinity_values()
# Strange().numeric_plot()
# temp_df = Strange().set_uncommon_categories_to_other(df, threshold=0.05)
# Strange().category_count(temp_df, nth_set_of_8=1)
# temp_df2 = Strange().set_uncommon_categories_to_other(df, threshold=0.01)
# Strange().print_best_fuzzy_scores(temp_df2, nth_set_of_8=1)
# Strange().excel_sorted_category_values_and_count(temp_df, nth_set_of_8=1)

### Ctrl Option - ###


def changepoint_plot(df, value_column):
    df_first_sixty_values = df[:60]
    my_breakpoints = rpt.Dynp(model="l1", min_size=1, jump=1).fit_predict(
        df[value_column].values, 1)
    initial_changepoint_index, _ = my_breakpoints
    rpt.show.display(df[value_column].values, my_breakpoints,
                     my_breakpoints, figsize=(10, 6))
    plt.show()


changepoint_plot(df, value_column)

### Ctrl Option - ###


def class_imbalance_plot(df):
    value_count_result = pd.DataFrame(pd.value_counts(
        df['y'].values, sort=False, normalize=True), columns=["y_pct"]).reset_index()
    value_count_result = value_count_result.sort_values(by=["y_pct"])
    print(value_count_result)
    print("Make sure no y values are half the size of any others")


class_imbalance_plot(df)
